{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Gradient Descent\n",
    "  \n",
    "We should mention that all the functions are **differentiable** so we can calculate their gradient.  \n",
    "  \n",
    "GD is an iterative method that for a given initial state and step size finds the value that minimizes the given function.\n",
    "  \n",
    "1) initial step: as we said GD is iterative method so we need to specify an initial step to start the iteration. We should notice that according to initial step, GD may converge to different minimums (locals or global).  \n",
    "  \n",
    "2) step size (or alpha): aplpha is a critical parameter in GD. small alpha may take so long to converge. and big alpha may result in oscilation over the minima. alpha is limited in the interval [0,1].  \n",
    "  \n",
    "**Note:** instead of using fixed value for alpha we can use backtracking or heuristic algorithms which computes an alpha that gaurantee the convergance at that specific iteration (this is not necessarly the best alpha).  \n",
    "  \n",
    "3) descent direction (or P): another key parameter of GD is Pk which is the descent direction of the given function. In order to always obtain decsent direction, we consider P = grad(f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
